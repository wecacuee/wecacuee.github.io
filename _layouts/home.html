---
layout: default
---

{%- if page.title -%}
  <h1 class="page-heading">{{ page.title }}</h1>
{%- endif -%}

{{ content }}

{%- if site.posts.size > 0 -%}
  <ul class="posts">
    <li class="posts-labelgroup" id="posts-labelgroup">
      <h1 id="posts-label">Updates</h1>
      {% if site.plainwhite.search %}
      <div class="search-container">
        <div class="search-section">
          <i class="icon-search"></i>
          <input type="text" name="search" id="searchbar" autocomplete="off" aria-label="search in posts">
        </div>
        <div class="search-results" id="search-results" data-placeholder="No Results" style="display: none;"></div>
      </div>
      {% endif %}
    </li>

    {%- for post in site.posts -%}
      <li style="clear:right">
        {%- assign date_format = site.plainwhite.date_format | default: "%b %-d, %Y" -%}
        <a class="post-link" href="{{ post.url | relative_url }}">
          <h2 class="post-title">{{ post.title | escape }}</h2>
        </a>
        <div class="post-meta">
          <div class="post-date">
            <i class="icon-calendar"></i>
            {{ post.date | date: date_format }}
          </div>
          {%- if post.categories.size > 0-%}
          <ul class="post-categories">
            {%- for tag in post.categories -%}
              <li><a href="/category/{{ tag }}">{{ tag }}</a></li>
            {%- endfor -%}
          </ul>
          {%- endif -%}
        </div>
        <div class="post">
          {%- if site.show_excerpts -%}
            {{ post.excerpt }}
          {%- endif -%}
          {%- if post.summary_image_url -%}
            <img src="{{ post.summary_image_url }}" />
          {%- endif -%}
        </div>
      </li>
    {%- endfor -%}

{%- endif -%}
    <li style="clear:right">
        <a href="https://arxiv.org/abs/1802.02274v1">
        <h2 class="post-title"> A Critical Investigation of Deep Reinforcement Learning for Navigation</h2>
        </a>
        <div class="post-meta">
            <div class="post-date">
                <i class="icon-calendar"></i>
                Jan 1, 2018
            </div>
            <ul class="post-categories">
                <li><a href="/category/papers">papers</a></li>
            </ul>
        </div>
        <div class="post">
            <!-- [Paper](https://arxiv.org/abs/1802.02274v1) https://openreview.net/forum?id=BkiIkBJ0b) 
                 &nbsp;|&nbsp;
            -->
        <a href="https://arxiv.org/abs/1802.02274v1">Paper</a>,
        <a href="https://github.com/umrobotslang/does-drl-learn-to-navigate">Code</a>


        <p>
            We find the Deep Reinforcement Learning for Navigation does not navigate on new maps.

        </p>
        <br/>
        <br/>
        <p>
            <style>
             img.float-right {
                 float:right;width:50%;min-width:200px
             }
            </style>
            <img class="float-right" src="images/critical-investigation.png" />
            The navigation problem is classically approached in two steps: an exploration step, where map-information about the environment is gathered; and an exploitation step, where this information is used to navigate efficiently. Deep reinforcement learning (DRL) algorithms, alternatively, approach the problem of navigation in an end-to-end fashion. Inspired by the classical approach, we ask whether DRL algorithms are able to inherently explore, gather and exploit map-information over the course of navigation. We build upon Mirowski et al. [2017] work and introduce a systematic suite of experiments that vary three parameters: the agent's starting location, the agent's target location, and the maze structure. We choose evaluation metrics that explicitly measure the algorithm's ability to gather and exploit map-information. Our experiments show that when trained and tested on the same maps, the algorithm successfully gathers and exploits map-information. However, when trained and tested on different sets of maps, the algorithm fails to transfer the ability to gather and exploit map-information to unseen maps. Furthermore, we find that when the goal location is randomized and the map is kept static, the algorithm is able to gather and exploit map-information but the exploitation is far from optimal. We open-source our experimental suite in the hopes that it serves as a framework for the comparison of future algorithms and leads to the discovery of robust alternatives to classical navigation methods. 
        </p>


        </div>
    </li>
    <li style="clear:right">
        <a href="">
            <h2 class="post-title">
                Lecture on Probabilistic Graphical Models
            </h2>
        </a>
        <div class="post-meta">
            <div class="post-date">
                <i class="icon-calendar"></i>
                Nov 9, 2017
            </div>
            <ul class="post-categories">
                <li><a href="/category/teaching">teaching</a></li>
            </ul>
        </div>
        <div class="post">
            A guest lecture on PGMs in <a href="http://web.eecs.umich.edu/~jjcorso/t/542W17/" >Advanced Topics in Computer Vision</a>
            <br/>
                <a src="http://www-personal.umich.edu/~dhiman/eecs442/20171109.html" >Presentation</a>
        </div>
    </li>


    <li style="clear:right">
        <a href="">
            <h2 class="post-title">
                A Continuous Occlusion Model for Road Scene Understanding 
            </h2>
        </a>
        <div class="post-meta">
            <div class="post-date">
                <i class="icon-calendar"></i>
                June 1, 2016
            </div>
            <ul class="post-categories">
                <li><a href="/category/papers">papers</a></li>
            </ul>
        </div>
        <div class="post">
            <a href="http://www-personal.umich.edu/~dhiman/images/continuousocclusion.pdf" >Paper</a>,
            <a href="http://www-personal.umich.edu/~dhiman/images/supplementarymaterial.pdf">Supplementary</a>,
            <a href="publications/mypub_bib.html#DhTrCoCVPR2016">Bibtex</a>


            <p>
                We propose to model cars as transparent ellipsoids rather cuboids to handle occlusion probabilistically.
            </p>

            <p>
                <img class="float-right" src="images/figure1.png" />
                We present a physically interpretable, continuous three-dimensional (3D) model for handling occlusions with applications to road scene understanding. We probabilistically assign each point in space to an object with a theoretical modeling of the reflection and transmission probabilities for the corresponding camera ray. Our modeling is unified in handling occlusions across a variety of scenarios, such as associating structure from motion (SFM) point tracks with potentially occluding objects or modeling object detection scores in applications such as 3D localization. For point track association, our model uniformly handles static and dynamic objects, which is an advantage over motion segmentation approaches traditionally used in multibody SFM. Detailed experiments on the KITTI raw dataset show the superiority of the proposed method over both state-of-the-art motion segmentation and a baseline that heuristically uses detection bounding boxes for resolving occlusions. We also demonstrate how our continuous occlusion model may be applied to the task of 3D localization in road scenes.
            </p>

        </div>
    </li>


    <li style="clear:right">
        <a href="">
            <h2 class="post-title">
                Pinhole camera workshop for middle school students
            </h2>
        </a>

            <div class="post-meta">
                <div class="post-date">
                    <i class="icon-calendar"></i>
                    June 25, 2015
                </div>
                <ul class="post-categories">
                    <li><a href="/category/outreach">outreach</a></li>
                </ul>
            </div>
            <div class="post">
                <a href="http://www-personal.umich.edu/~dhiman/xplore-workshop/pinhole.pdf">Presentation</a>

                <p>
                <a data-flickr-embed="true" data-footer="true" data-context="true" href="https://www.flickr.com/photos/xploreengineering/36593276425/in/album-72157685156089250/" title="Computer Imaging: Build Your Own Camera | Thursday June 25 | Session Two"><img src="https://live.staticflickr.com/4412/36593276425_a933de18b1.jpg" width="500" height="333" alt="Computer Imaging: Build Your Own Camera | Thursday June 25 | Session Two"></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script>
                </p>

            </div>
    </li>

    <li style="clear:right">
        <a href="">
            <h2 class="post-title">
                Modern MAP algorithms for occupancy grid mapping 
            </h2>
        </a>

            <div class="post-meta">
                <div class="post-date">
                    <i class="icon-calendar"></i>
                    June, 2014
                </div>
                <ul class="post-categories">
                    <li><a href="/category/papers">papers</a></li>
                </ul>
            </div>
            <div class="post">

                <p><a href="http://www-personal.umich.edu/~dhiman/images/modern_map.pdf">Paper</a>,
                    <a href="publications/mypub_bib.html">Bibtex</a>,
                    <a href="https://github.com/wecacuee/modern-occupancy-grid">Code</a></p>


                <p>
We propose to use modern graphical model inference techniques like Dual
Decomposition and Belief Propagation for occupancy grid mapping.
                </p>

                <p>
                    <img class="float-right" src="images/factorgraph3.svg"  />
Using the inverse sensor model has been popular in occupancy grid mapping. 
However, it is widely known that applying the inverse sensor model to mapping 
requires certain assumptions that are not necessarily true. Even the works
that use forward sensor models have relied on methods like expectation
maximization or Gibbs sampling which have been succeeded by more effective
methods of maximum a posteriori (MAP) inference over graphical models. In this
paper, we propose the use of modern MAP inference methods along with the
forward sensor model. Our implementation and experimental results demonstrate
that these modern inference methods deliver more accurate maps more
efficiently than previously used methods.
                </p>

            </div>
    </li>


    <li style="clear:right">
        <a href="">
            <h2 class="post-title">
                Voxel Planes 
            </h2>
        </a>
            <div class="post-meta">
                <div class="post-date">
                    <i class="icon-calendar"></i>
                    Nov, 2013
                </div>
                <ul class="post-categories">
                    <li><a href="/category/papers">papers</a></li>
                </ul>
            </div>
            <div class="post">

                <p><a href="http://www.cse.buffalo.edu/~jryde/publications/iros2013_voxel_planes.pdf">Paper</a>, <a href="publications/mypub_bib.html">Bibtex</a>, <a href="http://www-personal.umich.edu/~dhiman/voxelplanes/presentation.html">Presentation</a>, <a href="https://bitbucket.org/wecacuee/voxelplanes">Code</a></p>


                <p>
                    We propose using small planes inside voxels to model 3D surfaces and
                    corresponding algorithms to reconstruct them fast and accurately.
                </p>
                <p>
                    <img class="float-right" src="images/voxel_planes_90.jpg"  />
                    Our voxel planes approach first computes the PCA over the points inside a voxel, combining these PCA results across 2x2x2 voxel neighborhoods in a sliding window.  Second, the smallest eigenvector and voxel centroid define a plane which is intersected with the voxel to reconstruct the surface patch (3-6 sided convex polygon) within that voxel.  By nature of their construction these surface patches tessellate to produce a surface representation of the underlying points.
                </p>
                <p>
                    In experiments on public datasets the voxel planes method is 3 times faster than marching cubes, offers 300 times better compression than Greedy Projection, 10 fold lower error than marching cubes whilst allowing incremental map updates.
                </p>
            </div>

    </li>



    <li style="clear:right">
        <a href="">
            <h2 class="post-title">
                Kinfu based localization for Augmented Reality 
            </h2>
        </a>

        <div class="post-meta">
            <div class="post-date">
                <i class="icon-calendar"></i>
                Nov, 2012
            </div>
            <ul class="post-categories">
                <li><a href="/category/demos">demos</a></li>
            </ul>
        </div>
        <div class="post">
            <p>
                Used Kinect Fusion based camera localization from PCL library to render
                augmented reality (using VTK) that enables user to add markers to describe
                arbitrary objects in 3D.
            </p>

            <p>
                <div class="embed-responsive embed-responsive-16by9">
                <iframe class="embed-responsive-item" style="max-width:560px" height="315" src="//www.youtube-nocookie.com/embed/J62VZ7DGZfc" frameborder="0" allowfullscreen></iframe>
                </div>
            </p>
        </div>
    </li>
    <li style="clear:right">
        <a href="">
            <h2 class="post-title">
                Head tracking using RGBD at Hackathon 
            </h2>
        </a>

        <div class="post-meta">
            <div class="post-date">
                <i class="icon-calendar"></i>
                Oct, 2012
            </div>
            <ul class="post-categories">
                <li><a href="/category/demos">demos</a></li>
            </ul>
        </div>
        <div class="post">
            <p>

<p><a href="https://bitbucket.org/wecacuee/headtracker/">Code</a></p>
            </p>
            <p>
We used OpenCV face detector, KLT feature tracker, VTK visualizer to 
cook up a head tracker at UB Hackathon 2013 within 24 hours.
            </p>
<div class="embed-responsive embed-responsive-16by9">
<iframe class="embed-responsive-item" style="max-width:560px" height="315" src="//www.youtube-nocookie.com/embed/UlOsM9wbi3Y" frameborder="0" allowfullscreen></iframe>
</div>

        </div>
    </li>
    <li style="clear:right">
        <a href="">
            <h2 class="post-title">
                Mutual Localization 
            </h2>
        </a>

        <div class="post-meta">
            <div class="post-date">
                <i class="icon-calendar"></i>
                Nov, 2013
            </div>
            <ul class="post-categories">
                <li><a href="/category/papers">papers</a></li>
            </ul>
        </div>
        <div class="post">

            <p><a href="http://www.cse.buffalo.edu/~jryde/publications/iros2013_mutual_localization.pdf">Paper</a>, <a href="publications/mypub_bib.html">Bibtex</a>, <a href="http://www-personal.umich.edu/~dhiman/mutloc/presentation.html">Presentation</a>, <a href="https://mutual-localization.readthedocs.io/en/latest/intro.html">Code</a></p>
            <p>

We propose 3-point algorithm for two cameras to localize each other in 6-DOF. It
is a generalization of Perspective-3-Point algorithm when the 3-Points are
distributed and observed in two separate camera frames.
            </p>

            <p>
<img src="images/mathdiag.png" alt="img" class="float-right" />
Concurrently estimating the 6-DOF pose of multiple cameras or
robots---cooperative localization---is a core problem in contemporary
robotics.  Current works focus on a set of mutually observable world landmarks
and often require inbuilt egomotion estimates; situations in which both
assumptions are violated often arise, for example, robots with erroneous low
quality odometry and IMU exploring an unknown environment.  In contrast to
these existing works in cooperative localization, we propose a cooperative
localization method, which we call \textit{mutual localization}, that uses
reciprocal observations of camera-fiducials to obviate the need for egomotion
estimates and mutually observable world landmarks.  We formulate and solve an
algebraic formulation for the pose of the two camera mutual localization setup
under these assumptions.  Our experiments demonstrate the capabilities of our
proposal egomotion-free cooperative localization method: for example, the
method achieves 2cm range and 0.7 degree accuracy at 2m sensing for 6-DOF
pose.  To demonstrate the applicability of the proposed work, we deploy our
method on Turtlebots and we compare our results with ARToolKit and Bundler,
over which our method achieves a 10 fold improvement in translation
estimation accuracy.
            </p>

    <div class="embed-responsive embed-responsive-16by9">
    <iframe class="embed-responsive-item" style="max-width:560px" height="315" src="//www.youtube-nocookie.com/embed/9R546nuWPWA" frameborder="0" allowfullscreen></iframe>
    </div>
        </div>
    </li>

    <li style="clear:right">
        <a href="">
            <h2 class="post-title">
                Multi-Resolution Occupied Voxel Lists 
            </h2>
        </a>

        <div class="post-meta">
            <div class="post-date">
                <i class="icon-calendar"></i>
                Jan - Jun 2012
            </div>
            <ul class="post-categories">
                <li><a href="/category/demos">demos</a></li>
            </ul>
        </div>
        <div class="post">

<p>Added a few features like ROS support, support for color voxels to <a href="https://launchpad.net/mrol">MROL</a> library by Dr. Julian Ryde.</p>

<div class="embed-responsive embed-responsive-16by9">
<iframe class="embed-responsive-item" style="max-width:560px" height="315" src="//www.youtube-nocookie.com/embed/rJZJS2a9V_0" frameborder="0" allowfullscreen></iframe>
</div> 

        </div>
    </li>
    <li style="clear:right">
        <a href="">
            <h2 class="post-title">
                Work Experience  
            </h2>
        </a>

        <div class="post-meta">
            <div class="post-date">
                <i class="icon-calendar"></i>
                2008-2011
            </div>
            <ul class="post-categories">
                <li><a href="/category/demos">demos</a></li>
            </ul>
        </div>
        <div class="post">

At DE Shaw, I have worked on various small automation projects like Web robots, web scrapers, feeds parsers etc. Most of the these applications were written in Perl and a few in Python.

Apart from this I have also worked on a simple web application using J2EE. I used Hibernate, Struts 2 and Spring for the web application.
        </div>
    </li>


  </ul>
